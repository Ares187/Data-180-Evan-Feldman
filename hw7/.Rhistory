# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
install.packages('tidyverse')
library(tidyverse)
install.packages("tidyverse")
install.packages("cli")
install.packages("cli")
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
install.packages('tidyverse')
library(tidyverse)
library(tm)
news<-read.csv("D:/Dickinson College Teaching/DATA 180/hw7/news.csv",header=T)
# Chunk 3
posWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
install.packages("cli")
# Chunk 5
sort(unique(news$year))
# Chunk 6
charVector = news$headline_text
head(charVector)
# Chunk 7
wordVector = VectorSource(charVector)
wordCorpus = Corpus(wordVector)
# Chunk 8
wordCorpus = tm_map(wordCorpus, content_transformer(tolower))
wordCorpus = tm_map(wordCorpus, removePunctuation)
wordCorpus = tm_map(wordCorpus, removeNumbers)
wordCorpus = tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus[["1"]][["content"]]
# Chunk 9
tdm = TermDocumentMatrix(wordCorpus)
# Chunk 10
m = as.matrix(tdm)
wordCounts = rowSums(m)
wordCounts = sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
# Chunk 11
barplot(wordCounts[wordCounts>50], las = 2, cex.names = 0.75)
# Chunk 12
TotWords = sum(wordCounts)
positivePercent = match(names(wordCounts), posWords, nomatch = 0)
positivePercent = positivePercent != 0
positivePercent = wordCounts[positivePercent]
barplot(positivePercent[positivePercent>20], las = 2, cex.names = 0.75)
sum(positivePercent)/TotWords
# this equals 3.7 percent positive words
negativePercent = match(names(wordCounts), negWords, nomatch = 0)
negativePercent = negativePercent != 0
negativePercent = wordCounts[negativePercent]
barplot(negativePercent[negativePercent>20], las = 2, cex.names = 0.75)
sum(negativePercent)/TotWords
# this equals 7.7 percent negative words
Uniquewords = length(wordCounts)
Uniquewords
# Chunk 13
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
install.packages("cli")
install.packages("tidyverse")
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
install.packages('tidyverse')
library(tidyverse)
library(tm)
news<-read.csv("D:/Dickinson College Teaching/DATA 180/hw7/news.csv",header=T)
# Chunk 3
posWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
library("dplry")
install.packages("tidyverse")
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#install.packages('tidyverse')
library(tidyverse)
library(tm)
news<-read.csv("D:/Dickinson College Teaching/DATA 180/hw7/news.csv",header=T)
# Chunk 3
posWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
library("dplry")
# Chunk 1
# Custom options for knitting
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
error = FALSE,
fig.align = "center",
cache = FALSE
)
# Chunk 2
#install.packages('tidyverse')
library(tidyverse)
library(tm)
news<-read.csv("D:/Dickinson College Teaching/DATA 180/hw7/news.csv",header=T)
# Chunk 3
posWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/positive-words.txt", character(0), sep = "\n")  # 2006 items
negWords <- scan("D:/Dickinson College Teaching/DATA 180/hw7/negative-words.txt", character(0), sep = "\n")  # 4783 items
head(posWords,15)
head(negWords,15)
# Chunk 4
library("dplyr")
# Chunk 5
sort(unique(news$year))
# Chunk 6
charVector = news$headline_text
head(charVector)
# Chunk 7
wordVector = VectorSource(charVector)
wordCorpus = Corpus(wordVector)
# Chunk 8
wordCorpus = tm_map(wordCorpus, content_transformer(tolower))
wordCorpus = tm_map(wordCorpus, removePunctuation)
wordCorpus = tm_map(wordCorpus, removeNumbers)
wordCorpus = tm_map(wordCorpus, removeWords, stopwords("english"))
wordCorpus[["1"]][["content"]]
# Chunk 9
tdm = TermDocumentMatrix(wordCorpus)
# Chunk 10
m = as.matrix(tdm)
wordCounts = rowSums(m)
wordCounts = sort(wordCounts, decreasing = TRUE)
head(wordCounts, 10)
# Chunk 11
barplot(wordCounts[wordCounts>50], las = 2, cex.names = 0.75)
# Chunk 12
TotWords = sum(wordCounts)
positivePercent = match(names(wordCounts), posWords, nomatch = 0)
positivePercent = positivePercent != 0
positivePercent = wordCounts[positivePercent]
barplot(positivePercent[positivePercent>20], las = 2, cex.names = 0.75)
sum(positivePercent)/TotWords
# this equals 3.7 percent positive words
negativePercent = match(names(wordCounts), negWords, nomatch = 0)
negativePercent = negativePercent != 0
negativePercent = wordCounts[negativePercent]
barplot(negativePercent[negativePercent>20], las = 2, cex.names = 0.75)
sum(negativePercent)/TotWords
# this equals 7.7 percent negative words
Uniquewords = length(wordCounts)
Uniquewords
# Chunk 13
news <- news %>% group_by(year,month) %>% mutate(count=n(), yearmonth = paste(year, month,sep = '/')) %>% arrange(year,month,day)
news
# Chunk 14
# the number of articles released significantly declined over the years. in the beginning, there was an average of about 50 and in the middle of the years, there was a peak and massive decline afterwards. the right side of the graph is much lower.
ggplot(news, aes(x = factor(yearmonth, levels = unique(yearmonth)))) + geom_bar() + theme(axis.text = element_text(size = 4, angle = 90))
# Chunk 15
library("quanteda")
# Chunk 16
head(termFreq(charVector, control = list(removePunctuation = TRUE, stopwords = FALSE)), 20)
# Chunk 17
filter10 = function(x) {
x = tolower(x)
x = removePunctuation(x)
x = removeNumbers(x)
x = removeWords(x, stopwords('en'))
x = stripWhitespace(x)
return(x)
}
scrapecorp = Corpus(VectorSource(wordCorpus))
scrapecorp = tm_map(scrapecorp, content_transformer(filter10))
dtmss = DocumentTermMatrix(scrapecorp)
termfrequency = colSums(as.matrix(dtmss))
filteredTS = termfrequency[!(names(termfrequency) %in% stopwords("en"))]
top20 = head(sort(filteredTS, decreasing = TRUE), 20)
dtmpair = DocumentTermMatrix(scrapecorp, control = list(tokenize = function(x) ngram(words(x), n = 2)))
termfrequencypair = colSums(as.matrix(dtmpair))
filteredTSPair = termfrequencypair[!(names(termfrequencypair) %in% stopwords("en"))]
top20complete = head(sort(filteredTSPair, decreasing = TRUE), 40)
top20complete
# Chunk 18
newscorpus = corpus(charVector)
newscorpus = corpus_reshape(paragraphs, to = 'paragraphs')
